{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim import Adam, LBFGS\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from actnet.ActNet import ActNet\n",
    "from actnet.utils import adaptive_gradient_clipping, pinns_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOTA Comparisons\n",
    "\n",
    "### 2D Poisson PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 # BATCH_SIZE = 5000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "N_EPOCHS_TRAIN = 30000\n",
    "N_EPOCHS_FINE_TUNE = 100\n",
    "W = 32\n",
    "\n",
    "N_u = 100\n",
    "N_f = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f\"./data/poisson/poisson_w{W}.csv\")\n",
    "\n",
    "x = torch.Tensor(data[\"x\"])\n",
    "y = torch.Tensor(data[\"y\"])\n",
    "u_exact = torch.Tensor(data[\"u\"])\n",
    "\n",
    "X_star = torch.hstack((x.flatten().reshape(-1, 1), y.flatten().reshape(-1, 1)))\n",
    "\n",
    "boundary = torch.where(\n",
    "                (X_star[:, 0] == -1) | (X_star[:, 0] == 1) | (X_star[:, 1] == -1) | (X_star[:, 1] == 1)\n",
    "            )[0]\n",
    "indices = np.random.choice(boundary, N_u, replace=False)\n",
    "x_train = X_star[indices]\n",
    "u_train = u_exact[indices]\n",
    "\n",
    "indices = np.random.choice([i for i in range(len(X_star)) if i not in boundary], N_f, replace=False)\n",
    "x_train_f = X_star[indices]\n",
    "\n",
    "poisson_data = DataLoader(x_train_f, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 2\n",
    "d_out = 1\n",
    "d = 256\n",
    "m = 32\n",
    "N = 8\n",
    "L = 4\n",
    "\n",
    "model_poisson = ActNet(d_in, d_out, d, m, N, L).to(DEVICE)\n",
    "\n",
    "optimizer = Adam(model_poisson.parameters(), lr=1e-7)\n",
    "scheduler = SequentialLR(optimizer, [LinearLR(optimizer, 1e-7, 5e-3, 1000), StepLR(optimizer, 1000, 0.75)], [1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "model_poisson.train()\n",
    "for epoch in tqdm(range(N_EPOCHS_TRAIN)):\n",
    "    total_loss = 0\n",
    "    for inputs_f in poisson_data:\n",
    "        inputs, inputs_f, targets = x_train.to(DEVICE), inputs_f.to(DEVICE), u_train.to(DEVICE)\n",
    "        inputs_f.requires_grad = True\n",
    "\n",
    "        u_pred = model_poisson(inputs)\n",
    "        outputs_f = model_poisson(inputs_f)\n",
    "\n",
    "        u_pred = u_pred * (1 - inputs[:, 0]**2) * (1 - inputs[:, 1]**2)\n",
    "        outputs_f = outputs_f * (1 - inputs_f[:, 0]**2) * (1 - inputs_f[:, 1]**2)\n",
    "\n",
    "        u_grad = torch.autograd.grad(outputs_f, inputs_f, grad_outputs=torch.ones_like(outputs_f), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_grad[:, 0], inputs_f, grad_outputs=torch.ones_like(u_grad[:, 0]), create_graph=True)[0][:, 0]\n",
    "        u_yy = torch.autograd.grad(u_grad[:, 1], inputs_f, grad_outputs=torch.ones_like(u_grad[:, 1]), create_graph=True)[0][:, 1]\n",
    "        \n",
    "        f_pred = u_xx + u_yy - 2 * np.pi**2 * W**2 * torch.sin(np.pi * W * inputs_f[:, 0]) * torch.sin(np.pi * W * inputs_f[:, 1])\n",
    "\n",
    "        loss = pinns_loss(u_pred, targets, f_pred)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        adaptive_gradient_clipping(model_poisson.parameters())\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    writer.add_scalar(\"Loss/Adam/Poisson\", total_loss / len(poisson_data), epoch)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_closure():\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs, inputs_f, targets = x_train.to(DEVICE), x_train_f.to(DEVICE), u_train.to(DEVICE)\n",
    "    inputs_f.requires_grad = True\n",
    "\n",
    "    u_pred = model_poisson(inputs)\n",
    "    outputs_f = model_poisson(inputs_f)\n",
    "\n",
    "    u_pred = u_pred * (1 - inputs[:, 0]**2) * (1 - inputs[:, 1]**2)\n",
    "    outputs_f = outputs_f * (1 - inputs_f[:, 0]**2) * (1 - inputs_f[:, 1]**2)\n",
    "\n",
    "    u_xy = torch.autograd.grad(outputs_f, inputs_f, grad_outputs=torch.ones_like(outputs_f), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_xy[:, 0], inputs_f, grad_outputs=torch.ones_like(u_xy[:, 0]), create_graph=True)[0][:, 0]\n",
    "    u_yy = torch.autograd.grad(u_xy[:, 1], inputs_f, grad_outputs=torch.ones_like(u_xy[:, 1]), create_graph=True)[0][:, 1]\n",
    "    \n",
    "    f_pred = u_xx + u_yy - 2 * np.pi**2 * W**2 * torch.sin(np.pi * W * inputs_f[:, 0]) * torch.sin(np.pi * W * inputs_f[:, 1])\n",
    "\n",
    "    loss = pinns_loss(u_pred, targets, f_pred)\n",
    "    writer.add_scalar(\"Loss/LBFGS/Poisson\", loss.item(), epoch)\n",
    "\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "optimizer = LBFGS(model_poisson.parameters(), lr=1e-3, max_iter=100)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "for epoch in tqdm(range(N_EPOCHS_FINE_TUNE)):\n",
    "    optimizer.step(loss_closure)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poisson.eval()\n",
    "u_pred = model_poisson(X_star.to(DEVICE))\n",
    "\n",
    "print(\"L2 error: \", ((u_pred - u_exact)**2).mean())\n",
    "print(\"Relative L2 error: \", ((u_pred - u_exact)**2).mean() / (u_exact**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allen-Cahn PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 # BATCH_SIZE = 10000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "N_EPOCHS_TRAIN = 100000\n",
    "N_EPOCHS_FINE_TUNE = 100\n",
    "W = 32\n",
    "\n",
    "N_u = 100\n",
    "N_f = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"data/allen_cahn.mat\")\n",
    "usol = torch.Tensor(data[\"uu\"].flatten().reshape(-1, 1))\n",
    "\n",
    "t_star = torch.Tensor(data[\"tt\"][0])\n",
    "x_star = torch.Tensor(data[\"x\"][0])\n",
    "TT, XX = torch.meshgrid(t_star, x_star)\n",
    "\n",
    "X_star = torch.hstack((XX.flatten().reshape(-1, 1), TT.flatten().reshape(-1, 1)))\n",
    "\n",
    "boundary = torch.where(\n",
    "                (X_star[:, 0] == -1) | (X_star[:, 0] == 1) | (X_star[:, 1] == 0) | (X_star[:, 1] == 1)\n",
    "            )[0]\n",
    "indices = np.random.choice(boundary, N_u, replace=False)\n",
    "x_train = X_star[indices]\n",
    "u_train = usol[indices]\n",
    "\n",
    "indices = np.random.choice([i for i in range(len(X_star)) if i not in boundary], N_f, replace=False)\n",
    "x_train_f = X_star[indices]\n",
    "\n",
    "allen_cahn_data = DataLoader(x_train_f, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 2\n",
    "d_out = 1\n",
    "d = 256\n",
    "m = 80\n",
    "N = 8\n",
    "L = 4\n",
    "\n",
    "model_allen_cahn = ActNet(d_in, d_out, d, m, N, L).to(DEVICE)\n",
    "\n",
    "optimizer = Adam(model_allen_cahn.parameters(), lr=1e-7)\n",
    "scheduler = SequentialLR(optimizer, [LinearLR(optimizer, 1e-7, 5e-3, 1000), StepLR(optimizer, 1000, 0.9)], [1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "model_allen_cahn.train()\n",
    "for epoch in tqdm(range(N_EPOCHS_TRAIN)):\n",
    "    total_loss = 0\n",
    "    for inputs_f in allen_cahn_data:\n",
    "        inputs, inputs_f, targets = x_train.to(DEVICE), inputs_f.to(DEVICE), u_train.to(DEVICE)\n",
    "\n",
    "        inputs_f.requires_grad = True\n",
    "\n",
    "        u_pred = model_allen_cahn(inputs)\n",
    "        outputs_f = model_allen_cahn(inputs_f)\n",
    "\n",
    "        u_pred = (1 - inputs[:, 1]) * (inputs[:, 1]**2 * torch.cos(np.pi * inputs[:, 0])) + inputs[:, 1] * ((1 - inputs[:, 0]**2) * u_pred - 1)\n",
    "        outputs_f = (1 - inputs_f[:, 1]) * (inputs_f[:, 1]**2 * torch.cos(np.pi * inputs_f[:, 0])) + inputs_f[:, 1] * ((1 - inputs_f[:, 0]**2) * outputs_f - 1)\n",
    "\n",
    "        u_xt = torch.autograd.grad(outputs_f, inputs_f, grad_outputs=torch.ones_like(outputs_f), create_graph=True)[0]\n",
    "        u_t = u_xt[:, 1]\n",
    "        u_xx = torch.autograd.grad(u_xt[:, 0], inputs_f, grad_outputs=torch.ones_like(u_xt[:, 0]), create_graph=True)[0][:, 0]\n",
    "        \n",
    "        f_pred = u_t - 0.0001 * u_xx + 5 * (outputs_f**3 - outputs_f)\n",
    "\n",
    "        loss = pinns_loss(u_pred, targets, f_pred)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        adaptive_gradient_clipping(model_allen_cahn.parameters())\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    writer.add_scalar(\"Loss/Adam/Allen-Cahn\", total_loss / len(allen_cahn_data), epoch)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_closure():\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs, inputs_f, targets = x_train.to(DEVICE), x_train_f.to(DEVICE), u_train.to(DEVICE)\n",
    "    inputs_f.requires_grad = True\n",
    "\n",
    "    u_pred = model_allen_cahn(inputs)\n",
    "    outputs_f = model_allen_cahn(inputs_f)\n",
    "\n",
    "    u_pred = (1 - inputs[:, 1]) * (inputs[:, 1]**2 * torch.cos(np.pi * inputs[:, 0])) + inputs[:, 1] * ((1 - inputs[:, 0]**2) * u_pred - 1)\n",
    "    outputs_f = (1 - inputs_f[:, 1]) * (inputs_f[:, 1]**2 * torch.cos(np.pi * inputs_f[:, 0])) + inputs_f[:, 1] * ((1 - inputs_f[:, 0]**2) * outputs_f - 1)\n",
    "\n",
    "    u_xt = torch.autograd.grad(outputs_f, inputs_f, grad_outputs=torch.ones_like(outputs_f), create_graph=True)[0]\n",
    "    u_t = u_xt[:, 1]\n",
    "    u_xx = torch.autograd.grad(u_xt[:, 0], inputs_f, grad_outputs=torch.ones_like(u_xt[:, 0]), create_graph=True)[0][:, 0]\n",
    "    \n",
    "    f_pred = u_t - 0.0001 * u_xx + 5 * (outputs_f**3 - outputs_f)\n",
    "\n",
    "    loss = pinns_loss(u_pred, targets, f_pred)\n",
    "    writer.add_scalar(\"Loss/LBFGS/Allen-Cahn\", loss.item(), epoch)\n",
    "\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "optimizer = LBFGS(model_allen_cahn.parameters(), lr=1e-3, max_iter=100)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "for epoch in tqdm(range(N_EPOCHS_FINE_TUNE)):\n",
    "    optimizer.step(loss_closure)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_allen_cahn.eval()\n",
    "u_pred = model_allen_cahn(X_star.to(DEVICE)).detach().cpu()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.pcolor(TT, XX, u_pred, cmap=\"jet\")\n",
    "\n",
    "ax.set_xlabel(\"$t$\", size=20)\n",
    "ax.set_ylabel(\"$x$\", size=20)\n",
    "ax.set_title(\"Prediction\", fontsize=20) \n",
    "\n",
    "fig.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.pcolor(TT, XX, usol, cmap=\"jet\")\n",
    "\n",
    "ax.set_xlabel(\"$t$\", size=20)\n",
    "ax.set_ylabel(\"$x$\", size=20)\n",
    "ax.set_title(\"Reference Solution\", fontsize=20) \n",
    "\n",
    "fig.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.pcolor(TT, XX, (usol - u_pred).abs(), cmap=\"jet\")\n",
    "\n",
    "ax.set_xlabel(\"$t$\", size=20)\n",
    "ax.set_ylabel(\"$x$\", size=20)\n",
    "ax.set_title(\"Absolute Error\", fontsize=20) \n",
    "\n",
    "fig.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"L2 Error: \", torch.mean((usol - u_pred)**2))\n",
    "print(\"Relative L2 Error: \", torch.mean((usol - u_pred)**2) / torch.mean(usol**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burgers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "N_EPOCHS_TRAIN = 10000\n",
    "N_EPOCHS_FINE_TUNE = 100\n",
    "W = 32\n",
    "\n",
    "N_u = 100\n",
    "N_f = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"./data/burgers_shock.mat\")\n",
    "\n",
    "t = torch.Tensor(data[\"t\"].flatten())\n",
    "x = torch.Tensor(data[\"x\"].flatten())\n",
    "Exact = torch.Tensor(data[\"usol\"]).T\n",
    "\n",
    "T, X = torch.meshgrid(t, x)\n",
    "X_star = torch.hstack((X.flatten().reshape(-1, 1), T.flatten().reshape(-1, 1)))\n",
    "u_star = Exact.flatten()\n",
    "\n",
    "\n",
    "boundary = torch.where(\n",
    "                (X_star[:, 1] == 0) | (X_star[:, 0] == x.min()) | (X_star[:, 0] == x.max())\n",
    "            )[0]\n",
    "indices = np.random.choice(boundary, N_u, replace=False)\n",
    "x_train = X_star[indices, 0].reshape(-1, 1)\n",
    "t_train = X_star[indices, 1].reshape(-1, 1)\n",
    "u_train = u_star[indices].reshape(-1, 1)\n",
    "\n",
    "\n",
    "indices = np.random.choice(X_star.shape[0], N_f, replace=False)\n",
    "x_train_f = X_star[indices, 0].reshape(-1, 1)\n",
    "t_train_f = X_star[indices, 1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "x_train_f = torch.vstack((x_train_f, x_train))\n",
    "t_train_f = torch.vstack((t_train_f, t_train))\n",
    "\n",
    "\n",
    "def prepare(*tensors):\n",
    "    return (torch.Tensor(t).float().to(DEVICE) for t in tensors)\n",
    "\n",
    "x_train, t_train, u_train = prepare(x_train, t_train, u_train)\n",
    "x_train_f, t_train_f = prepare(x_train_f, t_train_f)\n",
    "\n",
    "x_star, t_star = prepare(*X_star.split((1, 1), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 2\n",
    "d_out = 1\n",
    "d = 256\n",
    "m = 32\n",
    "N = 8\n",
    "L = 4\n",
    "\n",
    "model_burgers = ActNet(d_in, d_out, d, m, N, L).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_closure():\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs_x, inputs_t, inputs_f_x, inputs_f_t, targets = x_train.to(DEVICE), t_train.to(DEVICE), x_train_f.to(DEVICE), t_train_f.to(DEVICE), u_train.to(DEVICE)\n",
    "\n",
    "    inputs_f_x.requires_grad = True\n",
    "    inputs_f_t.requires_grad = True\n",
    "\n",
    "    u_pred = model_burgers(torch.cat([inputs_x, inputs_t], dim=1))\n",
    "    outputs_f = model_burgers(torch.cat([inputs_f_x, inputs_f_t], dim=1))\n",
    "\n",
    "    df_dt = torch.autograd.grad(outputs_f, inputs_f_t, grad_outputs=torch.ones_like(outputs_f), create_graph=True)[0]\n",
    "    df_dx = torch.autograd.grad(outputs_f, inputs_f_x, grad_outputs=torch.ones_like(outputs_f), create_graph=True)[0]\n",
    "    df_dx2 = torch.autograd.grad(df_dx, inputs_f_x, grad_outputs=torch.ones_like(df_dx), create_graph=True)[0]\n",
    "    \n",
    "    f_pred = df_dt + outputs_f * df_dx - (0.01 / np.pi) * df_dx2\n",
    "\n",
    "    loss = pinns_loss(u_pred, targets, f_pred)\n",
    "    writer.add_scalar(\"Loss/LBFGS/Burgers\", loss.item(), epoch)\n",
    "\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "optimizer = LBFGS(model_burgers.parameters(), lr=1e-3, max_iter=100)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "model_burgers.train()\n",
    "for epoch in tqdm(range(N_EPOCHS_FINE_TUNE)):\n",
    "    optimizer.step(loss_closure)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "u_pred = model_burgers(torch.cat([x_star, t_star], dim=1)).detach().cpu()\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method=\"cubic\")\n",
    "\n",
    "h = ax.imshow(\n",
    "    U_pred.T,\n",
    "    interpolation=\"nearest\",\n",
    "    cmap=\"jet\",\n",
    "    extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"$t$\", size=20)\n",
    "ax.set_ylabel(\"$x$\", size=20)\n",
    "ax.legend(\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.9, -0.05),\n",
    "    ncol=5,\n",
    "    frameon=False,\n",
    "    prop={\"size\": 15},\n",
    ")\n",
    "ax.set_title(\"Prediction\", fontsize=20)\n",
    "ax.tick_params(labelsize=15)\n",
    "cbar = fig.colorbar(h)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h = ax.imshow(\n",
    "    Exact.T,\n",
    "    interpolation=\"nearest\",\n",
    "    cmap=\"jet\",\n",
    "    extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"$t$\", size=20)\n",
    "ax.set_ylabel(\"$x$\", size=20)\n",
    "ax.legend(\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.9, -0.05),\n",
    "    ncol=5,\n",
    "    frameon=False,\n",
    "    prop={\"size\": 15},\n",
    ")\n",
    "ax.set_title(\"Reference Solution\", fontsize=20)\n",
    "ax.tick_params(labelsize=15)\n",
    "cbar = fig.colorbar(h)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h = ax.imshow(\n",
    "    np.abs((U_pred.T - Exact.T.numpy())),\n",
    "    interpolation=\"nearest\",\n",
    "    cmap=\"jet\",\n",
    "    extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"$t$\", size=20)\n",
    "ax.set_ylabel(\"$x$\", size=20)\n",
    "ax.legend(\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.9, -0.05),\n",
    "    ncol=5,\n",
    "    frameon=False,\n",
    "    prop={\"size\": 15},\n",
    ")\n",
    "ax.set_title(\"Absolute Error\", fontsize=20)\n",
    "ax.tick_params(labelsize=15)\n",
    "cbar = fig.colorbar(h)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"L2 Error: \", np.mean((U_pred.T - Exact.T.numpy()).flatten()**2))\n",
    "print(\"Relative L2 Error: \", np.mean((U_pred.T - Exact.T.numpy()).flatten()**2) / np.mean(Exact.T.numpy().flatten()**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(d_in, d_out, d, m, N, L, batch_size, n_epochs_train, n_epochs_fine_tune, N_u, N_f, lambda_u, lambda_f):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = pd.read_csv(f\"./data/poisson/poisson_w16.csv\")\n",
    "\n",
    "    x = torch.Tensor(data[\"x\"])\n",
    "    y = torch.Tensor(data[\"y\"])\n",
    "    u_exact = torch.Tensor(data[\"u\"])\n",
    "\n",
    "    X_star = torch.hstack((x.flatten().reshape(-1, 1), y.flatten().reshape(-1, 1)))\n",
    "\n",
    "    boundary = torch.where(\n",
    "                    (X_star[:, 0] == -1) | (X_star[:, 0] == 1) | (X_star[:, 1] == -1) | (X_star[:, 1] == 1)\n",
    "                )[0]\n",
    "    indices = np.random.choice(boundary, N_u, replace=False)\n",
    "    x_train = X_star[indices]\n",
    "    u_train = u_exact[indices]\n",
    "\n",
    "    indices = np.random.choice([i for i in range(len(X_star)) if i not in boundary], N_f, replace=False)\n",
    "    x_train_f = X_star[indices]\n",
    "\n",
    "    poisson_data = DataLoader(x_train_f, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    model = ActNet(d_in, d_out, d, m, N, L).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=1e-7)\n",
    "    scheduler = SequentialLR(optimizer, [LinearLR(optimizer, 1e-7, 5e-3, 1000), StepLR(optimizer, 1000, 0.75)], [1000])\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(n_epochs_train)):\n",
    "        total_loss = 0\n",
    "        for inputs_f in poisson_data:\n",
    "            inputs, inputs_f, targets = x_train.to(device), inputs_f.to(device), u_train.to(device)\n",
    "            inputs_f.requires_grad = True\n",
    "\n",
    "            u_pred = model(inputs)\n",
    "            outputs_f = model(inputs_f)\n",
    "\n",
    "            u_pred = u_pred * (1 - inputs[:, 0]**2) * (1 - inputs[:, 1]**2)\n",
    "            outputs_f = outputs_f * (1 - inputs_f[:, 0]**2) * (1 - inputs_f[:, 1]**2)\n",
    "\n",
    "            u_grad = torch.autograd.grad(outputs_f, inputs_f, grad_outputs=torch.ones_like(outputs_f), create_graph=True)[0]\n",
    "            u_xx = torch.autograd.grad(u_grad[:, 0], inputs_f, grad_outputs=torch.ones_like(u_grad[:, 0]), create_graph=True)[0][:, 0]\n",
    "            u_yy = torch.autograd.grad(u_grad[:, 1], inputs_f, grad_outputs=torch.ones_like(u_grad[:, 1]), create_graph=True)[0][:, 1]\n",
    "            \n",
    "            f_pred = u_xx + u_yy - 2 * np.pi**2 * W**2 * torch.sin(np.pi * W * inputs_f[:, 0]) * torch.sin(np.pi * W * inputs_f[:, 1])\n",
    "\n",
    "            loss = pinns_loss(u_pred, targets, f_pred, lambda_u, lambda_f)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            adaptive_gradient_clipping(model.parameters())\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    def loss_closure():\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, inputs_f, targets = x_train.to(DEVICE), x_train_f.to(DEVICE), u_train.to(DEVICE)\n",
    "        inputs_f.requires_grad = True\n",
    "\n",
    "        u_pred = model(inputs)\n",
    "        outputs_f = model(inputs_f)\n",
    "\n",
    "        u_pred = u_pred * (1 - inputs[:, 0]**2) * (1 - inputs[:, 1]**2)\n",
    "        outputs_f = outputs_f * (1 - inputs_f[:, 0]**2) * (1 - inputs_f[:, 1]**2)\n",
    "\n",
    "        u_xy = torch.autograd.grad(outputs_f, inputs_f, grad_outputs=torch.ones_like(outputs_f), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_xy[:, 0], inputs_f, grad_outputs=torch.ones_like(u_xy[:, 0]), create_graph=True)[0][:, 0]\n",
    "        u_yy = torch.autograd.grad(u_xy[:, 1], inputs_f, grad_outputs=torch.ones_like(u_xy[:, 1]), create_graph=True)[0][:, 1]\n",
    "        \n",
    "        f_pred = u_xx + u_yy - 2 * np.pi**2 * W**2 * torch.sin(np.pi * W * inputs_f[:, 0]) * torch.sin(np.pi * W * inputs_f[:, 1])\n",
    "\n",
    "        loss = pinns_loss(u_pred, targets, f_pred)\n",
    "        writer.add_scalar(\"Loss/LBFGS/Poisson\", loss.item(), epoch)\n",
    "\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer = LBFGS(model.parameters(), lr=1e-3, max_iter=100)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    for epoch in tqdm(range(n_epochs_fine_tune)):\n",
    "        optimizer.step(loss_closure)\n",
    "    writer.close()\n",
    "\n",
    "    model.eval()\n",
    "    u_pred = model(X_star.to(device))\n",
    "    return ((u_pred - u_exact)**2).mean() / (u_exact**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Components\n",
    "\n",
    "### Influence of the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple problem without physics information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = {\n",
    "    \"Perceptron\": [],\n",
    "    \"MLP\": [],\n",
    "    \"ActNet\": []\n",
    "}\n",
    "\n",
    "test_losses = {\n",
    "    \"Perceptron\": [],\n",
    "    \"MLP\": [],\n",
    "    \"ActNet\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=5000, n_features=1, noise=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_reg_lin_data = DataLoader(TensorDataset(X_train, y_train), batch_size=300, shuffle=True)\n",
    "test_reg_lin_data = DataLoader(TensorDataset(X_test, y_test), batch_size=300, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"Simple Linear Regression Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = nn.Linear(1, 1).to(device)\n",
    "\n",
    "optimizer = Adam(perceptron.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(250)):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_reg_lin_data:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = perceptron(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses[\"Perceptron\"].append(total_loss / len(train_reg_lin_data))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_reg_lin_data:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = perceptron(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "        test_losses[\"Perceptron\"].append(total_loss / len(test_reg_lin_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.Sequential(\n",
    "    nn.Linear(1, 8),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(8, 8),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(8, 1)\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(250)):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_reg_lin_data:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses[\"MLP\"].append(total_loss / len(train_reg_lin_data))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_reg_lin_data:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = mlp(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "        test_losses[\"MLP\"].append(total_loss / len(test_reg_lin_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ActNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 1\n",
    "d_out = 1\n",
    "d = 128\n",
    "m = 128\n",
    "N = 8\n",
    "L = 2\n",
    "\n",
    "actnet = ActNet(d_in, d_out, d, m, N, L).to(device)\n",
    "optimizer = Adam(actnet.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actnet.train()\n",
    "for epoch in tqdm(range(250)):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_reg_lin_data:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = actnet(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        adaptive_gradient_clipping(actnet.parameters(), clip_factor=1e-5)\n",
    "        optimizer.step()\n",
    "    train_losses[\"ActNet\"].append(total_loss / len(train_reg_lin_data))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_reg_lin_data:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = actnet(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "        test_losses[\"ActNet\"].append(total_loss / len(test_reg_lin_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = list(train_losses.keys())\n",
    "n_models = len(model_names)\n",
    "\n",
    "plt.subplots(1, n_models, figsize=(6*n_models, 5), facecolor=\"#f5f5f5\")\n",
    "for i, model_name in enumerate(model_names):\n",
    "    plt.subplot(1, n_models, i+1)\n",
    "    plt.plot(train_losses[model_name], label=\"Apprentissage\")\n",
    "    plt.plot(test_losses[model_name], label=\"Test\")\n",
    "    plt.title(f\"{model_name} Loss Values\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    print(f\"{model_name} Loss - Train: {train_losses[model_name][-1]} - Test: {test_losses[model_name][-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
